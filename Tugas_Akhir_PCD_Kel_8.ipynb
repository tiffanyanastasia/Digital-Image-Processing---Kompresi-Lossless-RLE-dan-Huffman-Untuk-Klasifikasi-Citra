{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = '/content/drive/MyDrive/dataset_pcd/images.csv'\n",
        "df = pd.read_csv(path)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "RYfwGW3jzWDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Huffman**"
      ],
      "metadata": {
        "id": "FhC9K6MnzRKU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKmb-_gqyx3r"
      },
      "outputs": [],
      "source": [
        "import cv2, heapq, json, os\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, frequency, symbol, left=None, right=None):\n",
        "        self.frequency = frequency\n",
        "        self.symbol = symbol\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.huffman_direction = ''\n",
        "    def __lt__(self, nxt):\n",
        "        return self.frequency < nxt.frequency\n",
        "\n",
        "def calculate_huffman_codes(node, code='', huffman_codes=None):\n",
        "    if huffman_codes is None:\n",
        "        huffman_codes = {}\n",
        "    code += node.huffman_direction\n",
        "    if node.left:\n",
        "        calculate_huffman_codes(node.left, code, huffman_codes)\n",
        "    if node.right:\n",
        "        calculate_huffman_codes(node.right, code, huffman_codes)\n",
        "    if not node.left and not node.right:\n",
        "        huffman_codes[node.symbol] = code\n",
        "    return huffman_codes\n",
        "\n",
        "def get_merged_huffman_tree(byte_to_frequency):\n",
        "    huffman_tree = []\n",
        "    for byte, frequency in byte_to_frequency.items():\n",
        "        heapq.heappush(huffman_tree, Node(frequency, byte))\n",
        "    while len(huffman_tree) > 1:\n",
        "        left = heapq.heappop(huffman_tree)\n",
        "        right = heapq.heappop(huffman_tree)\n",
        "        left.huffman_direction = \"0\"\n",
        "        right.huffman_direction = \"1\"\n",
        "        merged_node = Node(left.frequency + right.frequency, left.symbol + right.symbol, left, right)\n",
        "        heapq.heappush(huffman_tree, merged_node)\n",
        "    return huffman_tree[0]\n",
        "\n",
        "def get_frequency(image_bit_string):\n",
        "    byte_to_frequency = {}\n",
        "    for i in range(0, len(image_bit_string), 8):\n",
        "        byte = image_bit_string[i:i+8]\n",
        "        byte_to_frequency[byte] = byte_to_frequency.get(byte, 0) + 1\n",
        "    return byte_to_frequency\n",
        "\n",
        "def get_compressed_image(image_bit_string, huffman_codes):\n",
        "    return ''.join(huffman_codes[image_bit_string[i:i+8]] for i in range(0, len(image_bit_string), 8))\n",
        "\n",
        "def compress_channel(channel):\n",
        "    bit_string = ''.join(format(pixel, '08b') for row in channel for pixel in row)\n",
        "    freq = get_frequency(bit_string)\n",
        "    tree = get_merged_huffman_tree(freq)\n",
        "    codes = calculate_huffman_codes(tree)\n",
        "    compressed_bits = get_compressed_image(bit_string, codes)\n",
        "    return compressed_bits, codes\n",
        "\n",
        "def decompress_channel(compressed_bits, codes, shape):\n",
        "    code_to_byte = {v: k for k, v in codes.items()}\n",
        "    bit_string = ''\n",
        "    current = ''\n",
        "    for bit in compressed_bits:\n",
        "        current += bit\n",
        "        if current in code_to_byte:\n",
        "            bit_string += code_to_byte[current]\n",
        "            current = ''\n",
        "    # convert bit string to image\n",
        "    h, w = shape\n",
        "    image = np.zeros((h, w), dtype=np.uint8)\n",
        "    idx = 0\n",
        "    for i in range(h):\n",
        "        for j in range(w):\n",
        "            byte = bit_string[idx:idx+8]\n",
        "            image[i, j] = int(byte, 2)\n",
        "            idx += 8\n",
        "    return image\n",
        "\n",
        "# ========== MAIN ==========\n",
        "image_folder = '/content/drive/MyDrive/PCD/images'\n",
        "output_folder = '/content/drive/MyDrive/PCD/huffman_result_rgb'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "compression_ratios = []\n",
        "\n",
        "for image_file in image_files:\n",
        "    image_path = os.path.join(image_folder, image_file)\n",
        "    image = cv2.imread(image_path)\n",
        "    h, w, _ = image.shape\n",
        "\n",
        "    # Pisah RGB dan quantisasi agar variasi warna tidak terlalu banyak\n",
        "    image = (image // 8) * 8\n",
        "    R, G, B = cv2.split(image)\n",
        "\n",
        "    compressed_R, codes_R = compress_channel(R)\n",
        "    compressed_G, codes_G = compress_channel(G)\n",
        "    compressed_B, codes_B = compress_channel(B)\n",
        "\n",
        "    total_bits = len(compressed_R) + len(compressed_G) + len(compressed_B)\n",
        "    original_bits = h * w * 3 * 8\n",
        "\n",
        "    # Dekompres dan gabungkan kembali\n",
        "    R_dec = decompress_channel(compressed_R, codes_R, (h, w))\n",
        "    G_dec = decompress_channel(compressed_G, codes_G, (h, w))\n",
        "    B_dec = decompress_channel(compressed_B, codes_B, (h, w))\n",
        "\n",
        "    reconstructed = cv2.merge([R_dec, G_dec, B_dec])\n",
        "    save_path = os.path.join(output_folder, f'{image_file}')\n",
        "    cv2.imwrite(save_path, reconstructed)\n",
        "\n",
        "    ratio = original_bits / total_bits\n",
        "    compression_ratios.append(ratio)\n",
        "\n",
        "    print(f\"{image_file} | Original: {original_bits} bits | Encoded: {total_bits} bits | Rasio: {ratio:.2f}\")\n",
        "\n",
        "# Rata-rata rasio kompresi\n",
        "if compression_ratios:\n",
        "    avg_ratio = sum(compression_ratios) / len(compression_ratios)\n",
        "    print(f\"\\nRata-rata rasio kompresi: {avg_ratio:.2f}\")\n",
        "else:\n",
        "    print(\"\\nTidak ada gambar yang diproses.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RLE**"
      ],
      "metadata": {
        "id": "S7vJvN9Hza6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Path\n",
        "csv_path = '/content/drive/MyDrive/PCD/images.csv'\n",
        "image_folder = '/content/drive/MyDrive/PCD/images'\n",
        "rle_output_folder = '/content/drive/MyDrive/PCD/images_rle'\n",
        "decoded_image_folder = '/content/drive/MyDrive/PCD/images_decoded'\n",
        "\n",
        "# Buat folder output jika belum ada\n",
        "os.makedirs(rle_output_folder, exist_ok=True)\n",
        "os.makedirs(decoded_image_folder, exist_ok=True)\n",
        "\n",
        "# Baca nama file dari CSV\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Fungsi RLE\n",
        "def run_length_encoding(data):\n",
        "    encoding = []\n",
        "    prev_pixel = data[0]\n",
        "    count = 1\n",
        "    for pixel in data[1:]:\n",
        "        if pixel == prev_pixel:\n",
        "            count += 1\n",
        "        else:\n",
        "            encoding.append((prev_pixel, count))\n",
        "            prev_pixel = pixel\n",
        "            count = 1\n",
        "    encoding.append((prev_pixel, count))\n",
        "    return encoding\n",
        "\n",
        "# Fungsi dekompresi RLE\n",
        "def run_length_decoding(rle_data, total_pixels):\n",
        "    flat_array = []\n",
        "    for item in rle_data.strip().split():\n",
        "        pixel, count = item.split(':')\n",
        "        flat_array.extend([int(pixel)] * int(count))\n",
        "    return np.array(flat_array[:total_pixels])  # crop jika lebih\n",
        "\n",
        "# Fungsi cari file gambar\n",
        "def find_image_file(base_name, folder):\n",
        "    for ext in ['.jpg', '.jpeg', '.png']:\n",
        "        path = os.path.join(folder, base_name + ext)\n",
        "        if os.path.exists(path):\n",
        "            return path\n",
        "    return None\n",
        "\n",
        "# Proses semua gambar\n",
        "for base_filename in df['image']:  # pastikan kolom 'image' sesuai\n",
        "    input_path = find_image_file(base_filename, image_folder)\n",
        "    if input_path is None:\n",
        "        print(f'File not found: {base_filename}')\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Buka gambar\n",
        "        with Image.open(input_path).convert('RGB') as img:\n",
        "            img_array = np.array(img)\n",
        "            shape = img_array.shape  # (H, W, 3)\n",
        "            h, w, c = shape\n",
        "\n",
        "            # Hitung ukuran asli (dalam bit)\n",
        "            original_bits = h * w * c * 8  # 8 bit per channel\n",
        "\n",
        "            # Kompresi per channel\n",
        "            rle_encoded = []\n",
        "            for ch in range(3):\n",
        "                flat_channel = img_array[:, :, ch].flatten()\n",
        "                encoded = run_length_encoding(flat_channel)\n",
        "                rle_encoded.append(encoded)\n",
        "\n",
        "            # Simpan ke file RLE\n",
        "            txt_filename = base_filename + '.txt'\n",
        "            rle_path = os.path.join(rle_output_folder, txt_filename)\n",
        "            with open(rle_path, 'w') as f:\n",
        "                f.write(f\"{h},{w},{c}\\n\")\n",
        "                for channel_encoded in rle_encoded:\n",
        "                    f.write(\" \".join(f\"{p}:{cnt}\" for p, cnt in channel_encoded) + '\\n')\n",
        "\n",
        "            # Hitung ukuran RLE (dalam bit)\n",
        "            compressed_bits = 0\n",
        "            for channel_encoded in rle_encoded:\n",
        "                for pixel, count in channel_encoded:\n",
        "                    # Simulasi penyimpanan: 8 bit untuk pixel + 16 bit untuk count\n",
        "                    compressed_bits += 8 + 16  # asumsi: pixel disimpan 8-bit, count 16-bit\n",
        "\n",
        "            compression_ratio = original_bits / compressed_bits if compressed_bits > 0 else 0\n",
        "            print(f'Compressed: {base_filename}')\n",
        "            print(f'  Original size: {original_bits} bits')\n",
        "            print(f'  Compressed size: {compressed_bits} bits')\n",
        "            print(f'  Compression ratio: {compression_ratio:.2f}')\n",
        "\n",
        "        # Dekompresi\n",
        "        with open(rle_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            shape = tuple(map(int, lines[0].strip().split(',')))\n",
        "            h, w, c = shape\n",
        "            decoded_channels = []\n",
        "            for line in lines[1:4]:\n",
        "                decoded_flat = run_length_decoding(line, h * w)\n",
        "                decoded_channels.append(decoded_flat.reshape((h, w)))\n",
        "\n",
        "            decoded_array = np.stack(decoded_channels, axis=2)\n",
        "            img_decoded = Image.fromarray(decoded_array.astype(np.uint8), mode='RGB')\n",
        "\n",
        "            decoded_path = os.path.join(decoded_image_folder, base_filename + '.jpg')\n",
        "            img_decoded.save(decoded_path)\n",
        "            print(f'Decoded & saved: {base_filename}.jpg\\n')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Failed: {base_filename} - {e}')\n"
      ],
      "metadata": {
        "id": "wAfbQB5Uzcde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODEL**"
      ],
      "metadata": {
        "id": "wVmxjzpkzqjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from skimage.feature import hog\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, learning_curve\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
        "\n",
        "# === Load Dataset ===\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/PCD/images.csv\")\n",
        "folder_ori = \"/content/drive/MyDrive/PCD/images\"\n",
        "folder_huff = \"/content/drive/MyDrive/PCD/huffman_result_rgb\"\n",
        "\n",
        "# === Load dan Ekstrak Fitur HOG ===\n",
        "def load_and_extract(folder, df):\n",
        "    features, labels = [], []\n",
        "    for _, row in df.iterrows():\n",
        "        path = os.path.join(folder, row['image'] + \".jpg\")\n",
        "        image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "        if image is None:\n",
        "            continue\n",
        "        image = cv2.resize(image, (128, 128))\n",
        "        hog_feature = hog(image, orientations=9, pixels_per_cell=(8, 8),\n",
        "                          cells_per_block=(2, 2), block_norm='L2-Hys', feature_vector=True)\n",
        "        features.append(hog_feature)\n",
        "        labels.append(row['name'])\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Gabungkan Original + Huffman\n",
        "X_ori, y_ori = load_and_extract(folder_ori, df)\n",
        "X_huff, y_huff = load_and_extract(folder_huff, df)\n",
        "X = np.concatenate([X_ori, X_huff], axis=0)\n",
        "y = np.concatenate([y_ori, y_huff], axis=0)\n",
        "\n",
        "# Encode Label\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# Cek distribusi kelas\n",
        "print(\"Distribusi label:\")\n",
        "print(pd.Series(y).value_counts())\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)\n",
        "\n",
        "# Model Random Forest\n",
        "clf = RandomForestClassifier(n_estimators=200, max_depth=20, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Prediksi & Evaluasi Dasar\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "print(f\"Akurasi: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n",
        "\n",
        "# === Cross-Validation Accuracy ===\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(clf, X, y_encoded, cv=skf, scoring='accuracy')\n",
        "print(f\"Cross-Validation Accuracy: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
        "\n",
        "# === ROC AUC (Multiclass OVR) ===\n",
        "if len(le.classes_) > 2:\n",
        "    y_prob = clf.predict_proba(X_test)\n",
        "    auc_score = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
        "    print(f\"Multiclass ROC AUC: {auc_score:.4f}\")\n",
        "else:\n",
        "    y_prob = clf.predict_proba(X_test)[:, 1]\n",
        "    auc_score = roc_auc_score(y_test, y_prob)\n",
        "    print(f\"ROC AUC: {auc_score:.4f}\")\n",
        "\n",
        "# === Learning Curve ===\n",
        "train_sizes, train_scores, val_scores = learning_curve(\n",
        "    clf, X, y_encoded, cv=5, scoring='accuracy',\n",
        "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
        ")\n",
        "\n",
        "train_mean = train_scores.mean(axis=1)\n",
        "val_mean = val_scores.mean(axis=1)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(train_sizes, train_mean, label=\"Train Accuracy\")\n",
        "plt.plot(train_sizes, val_mean, label=\"Validation Accuracy\")\n",
        "plt.xlabel(\"Train Size\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gvoaWKLMzlsF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}